{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215bc638-c901-426b-9224-c3f57f9efb3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7b635-ca9e-432f-b694-9a9bf8c3aed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Passive and active imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24c452-2edf-47c9-824f-23efbce4d10d",
   "metadata": {},
   "source": [
    "An image can be created through different means.\n",
    "- the object is the source of photon (SPECT, stars,...)\n",
    "- the object reflects/react to light given by a external source (flash, fluorescence)\n",
    "- the object is traversed by the ligh and diffuses/asborbes it (X-ray)\n",
    "\n",
    "|Illustration of the different ways of creating an image|\n",
    "|:-:|\n",
    "|<img src=\"output_3_0.png\" alt=\"Illustration of the different ways of creating an image\" title=\"Illustration of the different ways of creating an image\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5c4d0-137f-49e4-b4d6-c3d39165f056",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object as a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e865c-082e-44af-9d60-33032586d28b",
   "metadata": {},
   "source": [
    "A good example of imaging using an object as a source is nuclear imaging. An injection of radio-tracer accumulates to some region of interest, due to specific biochemical affinity, which then enables the imagery using the radio emmissions of said radio-tracer.\n",
    "> The following example shows how the radio-tracer identifies bone metastasis of a prostate cancer using a gamma camera.\n",
    "> \n",
    "> |Example of radio-tracing imagery|\n",
    "> |:-:|\n",
    "> |<img src=\"output_5_0.jpg\" alt=\"Example of radio-tracing imagery\" title=\"Example of radio-tracing imagery\" width=\"75\"/>|\n",
    "\n",
    "The source can also be the result of an external exitation, in other words an absorbtion and a re-emission of an other photon, like in fluorescence (fluorescence lymphography, fluorescence microscopy, ...)\n",
    "> In fluorescence lymphography, an infrared light beam is used to exite a fluorophore injected in the lymph system. The fluorophore can in turn re-emit infrared (at a longer wavelength). By using an adapted filter, one can observe the lymph displacement inside the lymph network (close to the skin surface).\n",
    ">\n",
    "> |Example of fluorescence lymphography|\n",
    "> |:-:|\n",
    "> |<img src=\"output_8_0.png\" alt=\"Example of fluorescence lymphography\" title=\"Example of fluorescence lymphography\" width=\"250\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a513c-09d3-4cdc-81e1-2b3447b8c25f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object reflects / diffuses the light from an external source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4960f03-59b1-44e4-8131-ec369591fa51",
   "metadata": {},
   "source": [
    "This is the more common acquisition setup : an external light source flood the scene with visible photons that are reflected by the objects, these photons are then acquired by a sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c81bc6-e852-4174-b121-2f2f63651c01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object attenuates the source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab7c0d-44ee-4cf6-b356-703e1bbd22d3",
   "metadata": {},
   "source": [
    "In this case, the source and the sensor are placed on both side of the object being imaged. \n",
    "> In x-ray imaging, an x-ray source projects some photons trough a patient, these photons interacting with the matter in such a way that tissue density and composition can give a contrast variation at the sensor level.\n",
    ">\n",
    "> |Example of x-ray imaging|\n",
    "> |:-:|\n",
    "> |<img src=\"output_14_0.jpg\" alt=\"Example of x-ray imaging\" title=\"Example of x-ray imaging\" width=\"250\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a80805-18d1-4891-9c4c-8fe4b4b052b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Direct image acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8097d-ca09-418b-b070-bcc3cbf738bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CCD - coupled charge device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb693de-8dab-4f09-b4c8-7c2203db768e",
   "metadata": {},
   "source": [
    "A charge-coupled device (CCD) is an integrated circuit containing an array of linked, or coupled, capacitors. Under the control of an external circuit, each capacitor can transfer its electric charge to a neighbouring capacitor. CCD sensors are a major technology used in digital imaging.\n",
    "\n",
    "|CCD charge transfer animation|CCD rectangular geometry|\n",
    "|:-:|:-:|\n",
    "|<img src = \"https://upload.wikimedia.org/wikipedia/commons/6/66/CCD_charge_transfer_animation.gif\" alt = \"CCD charge transfer animation\" title = \"CCD charge transfer animation\" width = \"400\" />|<img src = \"output_22_0.png\" alt = \"CCD rectangular geometry\" title = \"CCD rectangular geometry\" width = \"400\" />|\n",
    "\n",
    "Charges are liberated by light interacting with the semiconductor inside a photoactive region, for each pixel of the sensor grid. In order to digitize the amount of charges (proportional to the amount of captured light), the CCD device will move the charges along the substrate up to a charge to voltage converter.\n",
    "Charge-coupled devices use electrode potentials to move charges inside the silicium substrate as illustrated above.\n",
    "\n",
    "An image sensor can essentially have two types of geometry: \n",
    "- linear, typically used when the sensor is translated (flatbed scanner, but also bank note scanner, satellite, photo finish, ...), \n",
    "- rectangular, like almost every other camera.\n",
    "\n",
    "In order to move charges along the dimensions of the CCD sensor, charges are moved along each image line, a perpendicular buffer is used to discharge all these pixels in column into an amplifier that transform each charge into a voltage. The voltage is then converted by an ADC circuit. \n",
    "\n",
    "All the pixels charges being compared using the same circuit, the CCD sensor provides a very constant specification about the complete sensor. The other main advantage of the sensor is the coverage factor, which is the ratio between the sensor surface and the total pixel surface. Almost the whole surface is devoted to light acquisition, without any extra circuitry needed.\n",
    "    \n",
    "|Linear CCD sensor|CCD line sensor in a ceramic dual in-line package|\n",
    "|:-:|:-:|\n",
    "|<img src=\"output_24_0.jpg\" alt=\"Linear CCD sensor\" title=\"Linear CCD sensor\" width=\"400\"/>|<img src=\"output_26_0.jpg\" alt=\"Linear CCD sensor\" title=\"Linear CCD sensor\" width=\"200\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc523b8-a871-4bd2-a8e2-2fa20c4e23cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59be827-bcac-4225-a765-6eee4edf62d4",
   "metadata": {},
   "source": [
    "The CMOS technology embeds a photo-detector and a charge amplifier for each pixel of the sensor, the voltage then being transmitted by electrical conductors. This strategy enables a greater variety of sensor usage, for example adressing a part of the senor for lower resolution but higher speed. Because the conversion is done separately for each pixels, no charge shifting is needed, but discrepency between charge amplifier may exist, giving unequal pixel sensitivity and noise.\n",
    "\n",
    "|Illustration of the CMOS image sensor mechanism|\n",
    "|:-:|\n",
    "|<img src=\"output_29_0.png\" alt=\"Illustration of the CMOS image sensor mechanism\" title=\"Illustration of the CMOS image sensor mechanism\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e85c0-8553-44d7-9fe0-8404b9261e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CMOS vs CCD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357b59d-b63e-4694-bb05-3109468e8bda",
   "metadata": {},
   "source": [
    "A quick summary of the different caracteristics of the CDD and the CMOS technologies is reported in the next table. As one can see on the illustration below, both CMOS and CCD technologies have a high sensitivity to near infrared (NIR) wavelengths. Therefore, most of those kinds of sensors are equiped with a NIR filter. \n",
    "\n",
    "| Feature             | CCD             | CMOS     |\n",
    "|---------------------|-----------------|----------|\n",
    "| Signal out of pixel | Electron packet | Voltage  |\n",
    "| Fill factor         | high            | moderate |\n",
    "| Amplifier mismatch  | none            | moderate |\n",
    "| Noise               | low             | moderate |\n",
    "| system complexity   | high            | low      |\n",
    "| sensor complexity   | low             | high     |\n",
    "| dynamic range       | high            | moderate |\n",
    "| uniformity          | high            | moderate |\n",
    "| speed               | moderate        | high     |\n",
    "\n",
    "|Response of a silicon photodiode|\n",
    "|:-:|\n",
    "|<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Response_silicon_photodiode.svg/544px-Response_silicon_photodiode.svg.png\" alt=\"Response of a silicon photodiode\" title=\"Response of a silicon photodiode\" width=\"250\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2badc-ab57-4173-bc71-ef72da0e83a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multispectral acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0761deb-ee3f-4491-aa85-8a08b7cf07a0",
   "metadata": {},
   "source": [
    "Color acquisition is done by acquiring several images at different wavelength. One common and cheap approach is to cover the pixels of the sensor by colored dyes : red, green and blue. The figure below illustrates such filters, where on each 2x2 pixel square, one pixel is sensitive to the red part of the spectrum, one to the blue part of the spectrum, and finally two pixels are sensitive to the green part of the spectrum. The choice of duplicating green is done for symetry purposes, and also because the intensity sensitivity of the eye, which is related to the rods, is linked to the green part of the spectrum.\n",
    "\n",
    "|Illustration of a sensor with dyed pixels|\n",
    "|:-:|\n",
    "|<img src=\"output_36_0.png\" alt=\"Illustration of a sensor with dyed pixels\" title=\"Illustration of a sensor with dyed pixels\" width=\"250\"/>|\n",
    "\n",
    "One limitation of the dye approach is the resolution limitation : the image resolution is indeed divided by four (one pixel out of four is used for each color, two in the case of green). Another method used is based on three CCD devices coupled on the same optical axis and having three different dyes (red, green, blue) as illustrated bellow. The big advantage of this approach is to keep the sensor native resolution for each color channel.\n",
    "\n",
    "|Illustration of three coupled CCD devices with different dyes|\n",
    "|:-:|\n",
    "|<img src=\"output_39_0.png\" alt=\"Illustration of three coupled CCD devices with different dyes\" title=\"Illustration of three coupled CCD devices with different dyes\" width=\"250\"/>|\n",
    "> The number of spectral bands can be higher that three ! For example, satellite imagery offers many wavelength inside the three spectral bands but also outside, next to it (ultraviolet and near-infrared). Here are some examples.\n",
    "> - Quick-bird (envionemental imagery,  pixel = 0.65m)\n",
    "    - Pan: 450-900 nm\n",
    "    - Blue: 450-520 nm\n",
    "    - Green: 520-600 nm\n",
    "    - Red: 630-690 nm\n",
    "    - Near IR: 760-900 nm\n",
    "> - IKONOS (commercial earth observation satellite)\n",
    "    - Resolution :\n",
    "        - 0.8 m panchromatic (1-m PAN)\n",
    "        - 4-meter multispectral (4-m MS)\n",
    "    - Spectrum :\n",
    "        - Blue: 0.445–0.516 µm\n",
    "        - Green: 0.506–0.595 µm\n",
    "        - Red: 0.632–0.698 µm\n",
    "        - Near IR: 0.757–0.853 µm\n",
    "> - Landsat 8 (American Earth observation satellite)\n",
    "    - Band 1 - Coastal / Aerosol\t0.433 - 0.453 µm\t30 m\n",
    "    - Band 2 - Blue\t0.450 - 0.515 µm\t30 m\n",
    "    - Band 3 - Green\t0.525 - 0.600 µm\t30 m\n",
    "    - Band 4 - Red\t0.630 - 0.680 µm\t30 m\n",
    "    - Band 5 - Near Infrared\t0.845 - 0.885 µm\t30 m\n",
    "    - Band 6 - Short Wavelength Infrared\t1.560 - 1.660 µm\t30 m\n",
    "    - Band 7 - Short Wavelength Infrared\t2.100 - 2.300 µm\t30 m\n",
    "    - Band 8 - Panchromatic\t0.500 - 0.680 µm\t15 m\n",
    "    - Band 9 - Cirrus\t1.360 - 1.390 µm\t30 m\n",
    "> - AVIRIS - airborne visible/infrared imaging spectrometer \n",
    "    - four linear spectrometers (614-pixel wide) / 224 adjacent spectral bands.\n",
    "> |Illustration of the imagery proposed by the AVIRIS|\n",
    "> |:-:|\n",
    "> |<img src=\"output_42_0.jpg\" alt=\"Illustration of the imagery proposed by the AVIRIS\" title=\"Illustration of the imagery proposed by the AVIRIS\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba64597-25f7-444e-9433-9ff8a42e1954",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Depth acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c17359-623e-449e-babd-84bd6b42ee01",
   "metadata": {},
   "source": [
    "Depth imaging is traditionnaly used in stereo application, such for robot vision. Recently depth sensors became widely available thanks to game applications (Xbox Kinect). The main technologies used are the following :\n",
    "1. stereovision,\n",
    "2. laser triangulation,\n",
    "3. structured light projection,\n",
    "4. time-of-flight (TOF) imaging.\n",
    "\n",
    "The information provided by these sensors is of two kinds :\n",
    "1. an RGB image of the scene,\n",
    "2. a depth estimation, usually at a coarser resolution.\n",
    "\n",
    "|Example of a high resolution laser triangulation scanner|\n",
    "|:-:|\n",
    "|<img src=\"output_46_0.png\" alt=\"Example of a high resolution laser triangulation scanner\" title=\"Example of a high resolution laser triangulation scanner\" width=\"400\"/>|\n",
    "    \n",
    "When high speed is needed, structured light may be a solution. The depth is then computed by triangulation thanks to the identification of specific pattern in the image, like in the illustration below.\n",
    "> The first generation of the Kinect sensors uses the principe of structured light projection : a pseudo-random pattern is projected in the near-IR spectrum, thus invible to the human eye, and acquired by a IR sensitive camera. The depth image is produced with a video framerate compatible with gaming.\n",
    "> \n",
    "> |Picture of an Xbox Kinect|Illustration of the concept of structured light imaging|Picture of the pseudo-random near-IR projected pattern (red circles showing repeating elements in the pattern)|\n",
    "> |:-:|:-:|:-:|\n",
    "> |<img src=\"output_48_0.png\" alt=\"Picture of an Xbox Kinect\" title=\"Picture of an Xbox Kinect\" width=\"200\"/>|<img src=\"https://img.laserfocusworld.com/files/base/ebm/lfw/image/2016/01/pennwell.web.390.296.png?auto=format%2Ccompress&w=320\" alt=\"Illustration of the concept of structured light imaging\" title=\"Illustration of the concept of structured light imaging\" width=\"300\"/>|<img src=\"output_52_0.jpg\" alt=\"Picture of the pseudo-random near-IR projected pattern (red circles showing repeating elements in the pattern)\" title=\"Picture of the pseudo-random near-IR projected pattern (red circles showing repeating elements in the pattern)\" width=\"300\"/>|\n",
    "\n",
    "The second generation of sensors is based on a completely different technology, the time-of-flight (TOF). To estimate the distance between the sensor and the scene, a light wave is sent and received by the same sensor. The phase difference between a modulated light pattern sent by the source and the signal received by the camera gives a measure of the scene depth.\n",
    "\n",
    "It is possible to compute a distance with light with the concept of continuous wave demodulation. The computation follows this direction : Retrieval of the phase shift by demodulation of the received signal by cross-correlation with the emitted signal.\n",
    "\n",
    "|Illustration of the concept of TOF imaging|\n",
    "|:-:|\n",
    "|<img src=\"https://img.laserfocusworld.com/files/base/ebm/lfw/image/2016/01/pennwell.web.399.293.png?auto=format,compress&fit=max&height=684&width=1290.6000000000001\" alt=\"Illustration of the concept of TOF imaging\" title=\"Illustration of the concept of TOF imaging\" width=\"300\"/>\n",
    "\n",
    "1. Emitted signal : $$g(t) = \\cos(\\omega t)$$ with $\\omega$ the modulation frequency.\n",
    "2. Received signal (after the return trip to the scene surface) : $$s(t) = b + a \\cos(\\omega t +\\phi)$$ where $a$ is an unknown attenuation, $\\phi$ the phase shift(proportional to the scene distance) and $b$ an unknown acquisition noise (neglected here).\n",
    "\n",
    "|Emmitted and received signals, illustrating the phase shift between the both of them|\n",
    "|:-:|\n",
    "|<img src=\"output_61_0.png\" alt=\"Emmitted and received signals, illustrating the phase shift between the both of them\" title=\"Emmitted and received signals, illustrating the phase shift between the both of them\" width=\"400\"/>\n",
    "\n",
    "3. Cross correlation of both emitted and received signal : $$\\begin{align*} d(\\tau) &= s * g = \\int_{-\\infty}^{+\\infty} s(t).g(t+\\tau) dt \\\\ &= \\frac a 2 \\cos(\\omega t + \\phi) + b  \\end{align*}$$ with $\\tau$ an internal offset.\n",
    "\n",
    "4. Sampling the cross-correlation at four distinct moments : $$A_i = d(i \\cdot \\frac \\pi 2)  \\text{ with } i = 0,\\dots, 3$$ done to compute the phase offsets.\n",
    "\n",
    "|Phase offsets|\n",
    "|:-:|\n",
    "|<img src=\"output_64_0.png\" alt=\"Phase offsets\" title=\"Phase offsets\" width=\"400\"/>\n",
    "\n",
    "5. Phase and attenuation : $$\\begin{cases} \\phi = \\arg \\tan(\\frac{A_3-A_1}{A_0-A_2}) \\\\ a = \\frac 1 2 \\sqrt{(A_3-A_1)^2+(A_0-A_2)^2} \\end{cases}$$\n",
    "6. Scene distance : $$\\textrm{dist} = \\frac{c}{4.\\pi.\\omega} \\phi$$ with $c$ the speed of light.\n",
    "\n",
    "|Illustration of a depth image|\n",
    "|:-:|\n",
    "|<img src=\"output_67_0.jpg\" alt=\"Illustration of a depth image\" title=\"Illustration of a depth image\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7c724-048c-4e32-a4b8-2d7439fbc617",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indirect image acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8422a0d2-5c56-497d-9108-9776094e8512",
   "metadata": {},
   "source": [
    "An image can also be the result of a mathematical reconstruction based on an indirect acquisition : the sensor does not really acquire an image.\n",
    "> Computed tomography uses a series of 1D density profile acquisition enable a 2D reconstruction of the slice.\n",
    "> \n",
    "> |Illustration of the working of computed tomography|\n",
    "> |:-:|\n",
    "> |<img src=\"output_70_0.jpg\" alt=\"Illustration of the working of computed tomography\" title=\"Illustration of the working of computed tomography\" width=\"400\"/>\n",
    "> \n",
    "> Echography uses mechanical waves, their propagation being transformed in a 2D image showing the presence of interfaces between tissue of different acoustic impedence.\n",
    "> \n",
    "> |Illustration of an echography|\n",
    "> |:-:|\n",
    "> |<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/59/Ultrasound_of_human_heart_apical_4-cahmber_view.gif\" alt=\"Illustration of an echography\" title=\"Illustration of an echography\" width=\"400\"/>\n",
    "> \n",
    "> MRI image reconstruction is another good one of many examples of indirect imaging used in the medical field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea54974-586d-42e3-b4e6-58f190e169af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synthetic images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb9ebc-b940-4e14-80dc-f97e9d37c2e3",
   "metadata": {},
   "source": [
    "An image can also be the result of the grouping of a huge number of localized data.\n",
    "> One could imagine a network of temperature sensors spread over a complete country. The temperature measurements could then be grouped on a 2D map, and interpolated to have a complete coverage.\n",
    "\n",
    "Visualized data can be from various nature, the common aspect is that these data are placed in a geometric space (usually 2D or 3D)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
